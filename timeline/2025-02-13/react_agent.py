import os
import openai
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langgraph.store.memory import InMemoryStore
from langchain.memory import ConversationBufferMemory
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_core.tools import tool

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# âœ… LLM ì„¤ì • (GPT-4o ì‚¬ìš©)
llm = ChatOpenAI(model="gpt-4o-mini", max_tokens= 150)

# âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥ì†Œ (ëŒ€í™” ìƒíƒœ ì €ì¥)
store = InMemoryStore()
checkpointer = MemorySaver()

# âœ… ëŒ€í™” ê¸°ì–µì„ ìœ„í•œ ë©”ëª¨ë¦¬
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key="output")

# âœ… ê²€ìƒ‰ íˆ´ (ì‹¤ì‹œê°„ ì •ë³´ ì œê³µ)
search_tool = DuckDuckGoSearchRun()

# ğŸ”¹ **ëŒ€í™” ê¸°ì–µ íˆ´ (ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ)**
@tool
def memory_tool(query: str) -> str:
    """ì‚¬ìš©ìì˜ ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì„œ ëŒ€í™”í•˜ëŠ” íˆ´"""
    messages = memory.chat_memory.messages  # âœ… ì €ì¥ëœ ëŒ€í™” ê°€ì ¸ì˜¤ê¸°
    for msg in reversed(messages):
        if query.lower() in msg.content.lower():
            return msg.content  # âœ… ê·¸ëƒ¥ ìì—°ìŠ¤ëŸ½ê²Œ í•´ë‹¹ ë‚´ìš©ì„ ëŒ€í™”ì— ë…¹ì—¬ì„œ ì‚¬ìš©
    return ""

# ğŸ”¹ **ê²€ìƒ‰ íˆ´ (ì›¹ì—ì„œ ì •ë³´ ê²€ìƒ‰ í›„ ìš”ì•½)**
@tool
def search_summary_tool(query: str) -> str:
    """ì›¹ ê²€ìƒ‰ì„ í†µí•´ í•„ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” íˆ´"""
    search_results = search_tool.run(query)
    response = llm.invoke([
        {"role": "system", "content": "You are a helpful assistant that summarizes search results."},
        {"role": "user", "content": f"ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•´ì¤˜: {search_results}"}
    ])
    return response.content  # âœ… ë¶ˆí•„ìš”í•œ ë¼ë²¨ ì œê±°

# ğŸ”¹ **ìœ„ë¡œ & ê°ì • ë°˜ì‘ íˆ´ (ìì—°ìŠ¤ëŸ½ê²Œ ë§í•˜ê¸°)**
import random
@tool
def emotional_response_tool(user_input: str) -> str:
    """ì‚¬ìš©ìì˜ ê°ì •ì— ë”°ë¼ ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì‘í•˜ëŠ” íˆ´"""
    
    # ë¶€ì •ì ì¸ ê°ì • (í˜ë“¤ê±°ë‚˜ ìš°ìš¸í•  ë•Œ)
    if any(word in user_input.lower() for word in ["í˜ë“¤ì–´", "ìš°ìš¸í•´", "ìŠ¬í¼", "ì§€ì³¤ì–´", "ì§œì¦ë‚˜", "ì†ìƒí•´"]):
        responses = [
            "ì˜¤ëŠ˜ ë¬´ìŠ¨ ì¼ ìˆì—ˆì–´? ì–˜ê¸°í•´ ì¤„ë˜?",
            "ê´œì°®ì•„? ë¬´ìŠ¨ ì¼ ë•Œë¬¸ì— ê·¸ëŸ° ê¸°ë¶„ì´ ë“¤ì—ˆì–´?",
            "ìŒâ€¦ ë‚˜í•œí…Œ ë§í•´ë„ ê´œì°®ì•„. ë¬´ìŠ¨ ì¼ì¸ë°?",
            "ê·¸ë¬êµ¬ë‚˜... ë‚˜ë„ ê·¸ëŸ° ê¸°ë¶„ ë“¤ ë•Œê°€ ìˆì–´. ì¡°ê¸ˆì´ë¼ë„ í¸í•´ì§ˆ ìˆ˜ ìˆê²Œ ê°™ì´ ì´ì•¼ê¸°í• ê¹Œ?"
        ]
        return random.choice(responses)
    
    # ê¸ì •ì ì¸ ê°ì • (í–‰ë³µí•˜ê±°ë‚˜ ì‹ ë‚¬ì„ ë•Œ)
    elif any(word in user_input.lower() for word in ["ê¸°ë»", "ì¢‹ì•„", "í–‰ë³µí•´", "ì‹ ë‚˜", "ì„¤ë ˆ", "ì¦ê±°ì›Œ"]):
        responses = [
            "ì˜¤! ì¢‹ì€ ì¼ì´ ìˆì—ˆêµ¬ë‚˜! ë‚˜ë„ ê¶ê¸ˆí•œë°, ë¬´ìŠ¨ ì¼ì´ì•¼?",
            "ê·¸ê±° ì§„ì§œ ì¢‹ê² ë‹¤! ì–´ë–¤ ì¼ì¸ë°?",
            "ë“¤ìœ¼ë‹ˆê¹Œ ë‚˜ë„ ê¸°ë¶„ì´ ì¢‹ì•„ì§€ë„¤! ë” ì–˜ê¸°í•´ì¤„ë˜?",
            "ì™€, ë„ˆ ì •ë§ í–‰ë³µí•´ ë³´ì¸ë‹¤! ë¬´ìŠ¨ ì¼ì¸ì§€ ì•Œë ¤ì¤˜!"
        ]
        return random.choice(responses)

    # í™”ë‚¬ê±°ë‚˜ ì§œì¦ë‚  ë•Œ
    elif any(word in user_input.lower() for word in ["í™”ë‚˜", "ë¹¡ì³", "ì§œì¦", "ì—´ë°›ì•„", "ë‹µë‹µí•´"]):
        responses = [
            "ê·¸ë¬ì–´? ë§ì´ ì†ìƒí–ˆê² ë‹¤. ë¬´ìŠ¨ ì¼ ìˆì—ˆì–´?",
            "ì•„... ê·¸ëŸ° ìƒí™©ì´ë©´ ì§„ì§œ ë‹µë‹µí•  ê²ƒ ê°™ì•„. ì¢€ ë” ì–˜ê¸°í•´ ì¤„ë˜?",
            "ë„ˆë¬´ í™”ë‚˜ë©´ ì§„ì§œ í˜ë“¤ì§€... ë‚˜í•œí…Œë¼ë„ ì´ì•¼ê¸°í•´ë´.",
            "ê´œì°®ì•„, ì²œì²œíˆ ë§í•´ë„ ë¼. ì–´ë–¤ ì¼ì´ì•¼?"
        ]
        return random.choice(responses)

    # ê±±ì •í•˜ê±°ë‚˜ ê³ ë¯¼ì´ ìˆì„ ë•Œ
    elif any(word in user_input.lower() for word in ["ê±±ì •", "ê³ ë¯¼", "ë¶ˆì•ˆ", "ê±±ì •ë¼", "ì–´ë–¡í•˜ì§€"]):
        responses = [
            "ë­”ê°€ ê±±ì •ë˜ëŠ” ê²Œ ìˆêµ¬ë‚˜. ì–´ë–¤ ì¼ì´ì•¼?",
            "ê´œì°®ì•„, ì²œì²œíˆ ë§í•´ë„ ë¼. í˜¹ì‹œ ë‚´ê°€ ë„ì™€ì¤„ ìˆ˜ ìˆì„ê¹Œ?",
            "ê·¸ëŸ° ê³ ë¯¼ì´ ìˆìœ¼ë©´ ì •ë§ ì‹ ê²½ ì“°ì´ê² ë„¤. ë‚˜ë‘ ì´ì•¼ê¸°í•´ë³¼ë˜?",
            "ìŒ... ê·¸ëŸ¬ë©´ ì´ëŸ° ë°©ë²•ì€ ì–´ë•Œ? ê°™ì´ í•œë²ˆ ìƒê°í•´ë³´ì!"
        ]
        return random.choice(responses)

    return ""

# âœ… ì‚¬ìš© ê°€ëŠ¥í•œ íˆ´ ëª©ë¡
tools = [memory_tool, search_summary_tool, emotional_response_tool]

# âœ… ReAct ê¸°ë°˜ ì±—ë´‡ ìƒì„± (ëŒ€í™” ê¸°ì–µ + ê²€ìƒ‰ + ê°ì • ë°˜ì‘)
graph = create_react_agent(llm, tools=tools, checkpointer=checkpointer, store=store)

# âœ… ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ í•¨ìˆ˜ (ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µ)
def print_stream(graph, inputs, config):
    """ì²´í¬í¬ì¸íŠ¸ì™€ ë©”ëª¨ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ì§€ì†ì ì¸ ëŒ€í™”ë¥¼ ìœ ì§€"""
    for s in graph.stream(inputs, config, stream_mode="values"):
        message = s["messages"][-1]  
        if isinstance(message, tuple):
            print(message)  
        else:
            message.pretty_print()  

# âœ… ì±—ë´‡ ì‹¤í–‰ í•¨ìˆ˜
def run_agent():
    thread_id = input("\nâœ…ë‹¹ì‹ ì˜ ë‹‰ë„¤ì„ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    config = {"configurable": {"thread_id": thread_id}}
    graph.get_state(config)

    while True:
        user_input = input("\nì‚¬ìš©ì ì…ë ¥: ").strip()
        if user_input.lower() in ["ê·¸ë§Œ", "ì¢…ë£Œ", "quit"]:
            print("ğŸ›‘ ëŒ€í™” ì¢…ë£Œ")
            break

        # âœ… LLM ì‹¤í–‰ í›„ ì‘ë‹µ ì €ì¥
        inputs = {"messages": [("user", user_input)]}
        response = graph.invoke(inputs, config=config)

        # âœ… LangChain ë©”ëª¨ë¦¬ì— ì‚¬ìš©ì ì…ë ¥ ë° AI ì‘ë‹µ ì €ì¥
        memory.save_context({"input": user_input}, {"output": response["messages"][-1].content})

        print_stream(graph, inputs, config)

# âœ… ì‹¤í–‰
if __name__ == "__main__":
    run_agent()
