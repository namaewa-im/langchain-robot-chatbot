import os
import openai
import streamlit as st
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langgraph.store.memory import InMemoryStore
from langchain.memory import ConversationBufferMemory
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_core.tools import tool
import random

# âœ… Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ğŸ¤– AI ì±—ë´‡", layout="wide")

# âœ… ì™¼ìª½ ì‚¬ì´ë“œë°” (API Key & Thread ID ì…ë ¥)
with st.sidebar:
    st.title("ğŸ”‘ ì„¤ì •")
    openai_api_key = st.text_input("OpenAI API Key", type="password")
    thread_id = st.text_input("ğŸ†” ëŒ€í™” Thread ID (ì˜ˆ: user1)")

# âœ… ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥
if not openai_api_key or not thread_id:
    st.warning("âš ï¸ OpenAI API Keyì™€ Thread IDë¥¼ ì…ë ¥í•˜ì„¸ìš”!")
    st.stop()

# âœ… LLM ì„¤ì •
llm = ChatOpenAI(model="gpt-4o-mini", max_tokens=150, api_key=openai_api_key)

# âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥ì†Œ (ëŒ€í™” ìƒíƒœ ì €ì¥)
store = InMemoryStore()
checkpointer = MemorySaver()

# âœ… **LangChain ëŒ€í™” ë©”ëª¨ë¦¬**
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key="output")

# âœ… ê²€ìƒ‰ íˆ´ (ì‹¤ì‹œê°„ ì •ë³´ ì œê³µ)
search_tool = DuckDuckGoSearchRun()

# ğŸ”¹ **ëŒ€í™” ê¸°ì–µ íˆ´ (ì´ì „ ëŒ€í™” í™œìš©)**
@tool
def memory_tool(query: str) -> str:
    """ì‚¬ìš©ìì˜ ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì„œ ëŒ€í™”í•˜ëŠ” íˆ´"""
    messages = memory.load_memory_variables({})["chat_history"]
    return "\n".join([f"{msg.role}: {msg.content}" for msg in messages]) if messages else "ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."

# ğŸ”¹ **ê²€ìƒ‰ íˆ´**
@tool
def search_summary_tool(query: str) -> str:
    """ì›¹ ê²€ìƒ‰ì„ í†µí•´ í•„ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” íˆ´"""
    search_results = search_tool.run(query)
    response = llm.invoke([
        {"role": "system", "content": "You are a helpful assistant that summarizes search results."},
        {"role": "user", "content": f"ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•´ì¤˜: {search_results}"}
    ])
    return response.content

# ğŸ”¹ **ê°ì • ë°˜ì‘ íˆ´**
@tool
def emotional_response_tool(user_input: str) -> str:
    """ì‚¬ìš©ìì˜ ê°ì •ì— ë”°ë¼ ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì‘í•˜ëŠ” íˆ´"""
    if any(word in user_input.lower() for word in ["í˜ë“¤ì–´", "ìš°ìš¸í•´", "ìŠ¬í¼", "ì§€ì³¤ì–´", "ì§œì¦ë‚˜", "ì†ìƒí•´"]):
        return random.choice(["ê´œì°®ì•„? ë¬´ìŠ¨ ì¼ ìˆì—ˆì–´?", "ìŒâ€¦ ë‚˜í•œí…Œ ë§í•´ë„ ê´œì°®ì•„. ë¬´ìŠ¨ ì¼ì¸ë°?", "ê·¸ë¬êµ¬ë‚˜... ë‚˜ë„ ê·¸ëŸ° ê¸°ë¶„ ë“¤ ë•Œê°€ ìˆì–´."])
    elif any(word in user_input.lower() for word in ["ê¸°ë»", "ì¢‹ì•„", "í–‰ë³µí•´", "ì‹ ë‚˜", "ì„¤ë ˆ", "ì¦ê±°ì›Œ"]):
        return random.choice(["ì˜¤! ì¢‹ì€ ì¼ì´ ìˆì—ˆêµ¬ë‚˜! ë¬´ìŠ¨ ì¼ì´ì•¼?", "ì™€, ë„ˆ ì •ë§ í–‰ë³µí•´ ë³´ì¸ë‹¤!"])
    return ""

# âœ… ì‚¬ìš© ê°€ëŠ¥í•œ íˆ´ ëª©ë¡
tools = [memory_tool, search_summary_tool, emotional_response_tool]

# âœ… **ReAct ê¸°ë°˜ ì±—ë´‡ ìƒì„± (LangChain Memory ì ìš©)**
graph = create_react_agent(llm, tools=tools, checkpointer=checkpointer, store=store)

# âœ… **Streamlit ì„¸ì…˜ ìƒíƒœ ì €ì¥ (ì´ì „ ëŒ€í™” ìœ ì§€)**
if "messages" not in st.session_state:
    st.session_state.messages = []

# âœ… **ì±„íŒ… UI (í•­ìƒ ë³´ì´ë„ë¡ ìœ ì§€)**
st.title("ğŸ¤– AI ì±—ë´‡")
st.markdown("ğŸ’¬ **OpenAI ê¸°ë°˜ AI ì±—ë´‡ê³¼ ëŒ€í™”í•˜ì„¸ìš”!**")

# âœ… **ê¸°ì¡´ ì±„íŒ… ë©”ì‹œì§€ ì¶œë ¥**
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# âœ… **ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°**
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")

if user_input:
    # âœ… **ì´ì „ ëŒ€í™” ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸°**
    chat_history = memory.load_memory_variables({})["chat_history"]
    
    # âœ… **LLM ì‹¤í–‰ì„ ìœ„í•œ ë©”ì‹œì§€ êµ¬ì„±**
    messages = chat_history + [("user", user_input)]
    inputs = {"messages": messages}
    config = {"configurable": {"thread_id": thread_id}}

    # âœ… **LangGraph ì‹¤í–‰**
    response = graph.invoke(inputs, config=config)
    ai_response = response["messages"][-1].content

    # âœ… **LangChain Memoryì— ëŒ€í™” ì €ì¥**
    memory.save_context({"input": user_input}, {"output": ai_response})

    # âœ… **Streamlit ì„¸ì…˜ì—ë„ ì €ì¥**
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    st.session_state.messages.append({"role": "assistant", "content": ai_response})
    with st.chat_message("assistant"):
        st.markdown(ai_response)
