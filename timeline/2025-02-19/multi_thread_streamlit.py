import os
import json
import openai
import streamlit as st
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langgraph.store.memory import InMemoryStore
from langchain.memory import ConversationBufferMemory
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.agents import initialize_agent, AgentType
from langchain_core.tools import tool
import random

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# âœ… JSON íŒŒì¼ì—ì„œ ëŒ€í™” ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ (ì‚¬ìš©ìë³„ Thread ê´€ë¦¬)
def load_json_to_memory(memory, thread_id, filename):
    """JSON íŒŒì¼ì—ì„œ íŠ¹ì • ì‚¬ìš©ìì˜ ëŒ€í™” ë‚´ì—­ì„ ë¶ˆëŸ¬ì™€ ConversationBufferMemoryì— ì €ì¥"""
    try:
        with open(filename, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        if thread_id not in data:
            print(f"ì‚¬ìš©ì {thread_id}ì— ëŒ€í•œ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        for conversation in data[thread_id]:
            user_input = conversation.get("user", "")
            agent_response = conversation.get("agent", "")
            if user_input and agent_response:
                memory.save_context({"input": user_input}, {"output": agent_response})
        print(f"âœ… {filename}ì—ì„œ ì‚¬ìš©ì {thread_id}ì˜ ëŒ€í™” ë‚´ì—­ì´ ë©”ëª¨ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except FileNotFoundError:
        print(f"âŒ {filename} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except json.JSONDecodeError:
        print(f"âŒ {filename} íŒŒì¼ì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")

# âœ… JSON íŒŒì¼ì— ëŒ€í™” ë‚´ì—­ ì €ì¥ í•¨ìˆ˜
def save_memory_to_json(user_input, agent_response, thread_id, filename):
    """ì‚¬ìš©ìì˜ ëŒ€í™” ë‚´ì—­ì„ JSON íŒŒì¼ì— ì¶”ê°€ ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""

    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ (íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ì½ê³ , ì—†ìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ìƒì„±)
    data = {}
    if os.path.exists(filename):
        with open(filename, "r", encoding="utf-8") as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError:
                data = {}

    # í•´ë‹¹ ì‚¬ìš©ì(thread_id)ì˜ ëŒ€í™” ê¸°ë¡ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    if thread_id not in data:
        data[thread_id] = []

    # ìƒˆë¡œìš´ ëŒ€í™” ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)
    new_entry = {"user": user_input, "agent": agent_response}
    if new_entry not in data[thread_id]:  # ì¤‘ë³µ ë°©ì§€
        data[thread_id].append(new_entry)

    # JSON íŒŒì¼ì— ì €ì¥
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

    print(f"âœ… ì‚¬ìš©ì '{thread_id}'ì˜ ëŒ€í™” ë‚´ì—­ì´ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# âœ… Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ğŸ¤– AI ì±—ë´‡", layout="wide")

# âœ… ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.title("ğŸ”‘ ì„¤ì •")
    openai_api_key = st.text_input("OpenAI API Key", type="password")
    thread_id = st.text_input("User ID")
    filename = st.text_input("Create file name")
    filename = f'{filename}.json'

if not openai_api_key or not thread_id or not filename:
    st.warning("âš ï¸ OpenAI API Keyì™€ IDì™€ ëŒ€í™”ë¥¼ ì €ì¥í•  íŒŒì¼ëª…ì„ ì…ë ¥í•˜ì„¸ìš”!")
    st.stop()



# âœ… LLM ì„¤ì •
llm = ChatOpenAI(model="gpt-4o-mini", max_tokens=200, api_key=openai_api_key)

# âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥ì†Œ
store = InMemoryStore()
checkpointer = MemorySaver()

# âœ… LangChain Memory ì„¤ì •
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key="output")
load_json_to_memory(memory, thread_id, filename=filename)  # JSONì—ì„œ ëŒ€í™” ë¶ˆëŸ¬ì˜¤ê¸°

# âœ… ê²€ìƒ‰ íˆ´
search_tool = DuckDuckGoSearchRun()

# @tool
# def memory_tool(query: str) -> str:
#     """ì‚¬ìš©ìì˜ ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì„œ ëŒ€í™”í•˜ëŠ” íˆ´"""
#     messages = memory.load_memory_variables({}).get("chat_history", "")
#     return "\n".join(messages) if messages else "ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."

# @tool
# def memory_tool(query: str) -> str:
#     """ì‚¬ìš©ìì˜ ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì„œ ëŒ€í™”í•˜ëŠ” íˆ´"""
#     messages = memory.load_memory_variables({}).get("chat_history", [])
    
#     # ë©”ì‹œì§€ ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
#     messages_str = [msg.content for msg in messages]  # âœ… HumanMessage, AIMessage ê°ì²´ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

#     return "\n".join(messages_str) if messages_str else "ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."

@tool
def memory_tool(query: str) -> dict:
    """ì‚¬ìš©ìì˜ ì´ì „ ëŒ€í™”ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì—¬ AIê°€ ì‰½ê²Œ ì´í•´í•˜ë„ë¡ í•¨"""
    messages = memory.load_memory_variables({}).get("chat_history", [])
    messages_data = [{"role": msg.type, "content": msg.content} for msg in messages]

    return {"history": messages_data} if messages_data else {"history": []}


@tool
def search_summary_tool(query: str) -> str:
    """ì›¹ ê²€ìƒ‰ì„ í†µí•´ í•„ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” íˆ´"""
    search_results = search_tool.run(query)
    response = llm.invoke([
        {"role": "system", "content": "You are a helpful assistant that summarizes search results."},
        {"role": "user", "content": f"ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•´ì¤˜: {search_results}"}
    ])
    return response.content

@tool
def emotional_response_tool(user_input: str) -> str:
    """ì‚¬ìš©ìì˜ ê°ì •ì— ë”°ë¼ ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì‘í•˜ëŠ” íˆ´"""
    if any(word in user_input.lower() for word in ["í˜ë“¤ì–´", "ìš°ìš¸í•´", "ìŠ¬í¼", "ì§€ì³¤ì–´", "ì§œì¦ë‚˜"]):
        return random.choice(["ê´œì°®ì•„? ë¬´ìŠ¨ ì¼ ìˆì—ˆì–´?", "ìŒâ€¦ ë‚˜í•œí…Œ ë§í•´ë„ ê´œì°®ì•„. ë¬´ìŠ¨ ì¼ì¸ë°?"])
    elif any(word in user_input.lower() for word in ["ê¸°ë»", "ì¢‹ì•„", "í–‰ë³µí•´", "ì‹ ë‚˜"]):
        return random.choice(["ì˜¤! ì¢‹ì€ ì¼ì´ ìˆì—ˆêµ¬ë‚˜!", "ì™€, ë„ˆ ì •ë§ í–‰ë³µí•´ ë³´ì¸ë‹¤!"])
    return ""

# âœ… ReAct ê¸°ë°˜ ì±—ë´‡ ìƒì„±
tools=[memory_tool, search_summary_tool, emotional_response_tool]

# agent = create_react_agent(llm, tools = tools, 
#                            checkpointer=checkpointer, store=store,
#                            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # LLMì´ Taskë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ Toolì„ ì„ íƒ
#                            verbose=True)
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, 
    verbose=True,  
    handle_parsing_errors=True,
)


# âœ… ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ í•¨ìˆ˜ (ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µ)
def print_stream(graph, inputs, config):
    """ì²´í¬í¬ì¸íŠ¸ì™€ ë©”ëª¨ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ì§€ì†ì ì¸ ëŒ€í™”ë¥¼ ìœ ì§€"""
    for s in graph.stream(inputs, config, stream_mode="values"):
        message = s["messages"][-1]  
        if isinstance(message, tuple):
            print(message)  
        else:
            message.pretty_print()  

# âœ… Streamlit UI ìƒì„±
st.title("ğŸ¤– AI ì±—ë´‡")

# âœ… Streamlitì˜ ì„¸ì…˜ ìƒíƒœì—ì„œ thread_id ë³„ë¡œ ëŒ€í™” ë‚´ì—­ ê´€ë¦¬
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = {}  # ëª¨ë“  thread_idì˜ ëŒ€í™” ì €ì¥

# âœ… ì‚¬ìš©ìê°€ ìƒˆë¡œìš´ User IDë¥¼ ì…ë ¥í–ˆì„ ë•Œ ì²˜ë¦¬
if thread_id != st.session_state.get("thread_id", None):
    # âœ… ê¸°ì¡´ IDì˜ ëŒ€í™” ë‚´ì—­ ì €ì¥
    if st.session_state.get("thread_id") is not None:
        st.session_state["chat_history"][st.session_state["thread_id"]] = st.session_state["messages"]

    # âœ… ìƒˆ thread_idë¡œ ë³€ê²½
    st.session_state["thread_id"] = thread_id

    # âœ… ìƒˆ IDì˜ ëŒ€í™” ë‚´ì—­ì„ ë¶ˆëŸ¬ì˜¤ê±°ë‚˜ ì´ˆê¸°í™”
    if thread_id in st.session_state["chat_history"]:
        st.session_state["messages"] = st.session_state["chat_history"][thread_id]
    else:
        st.session_state["messages"] = []  # ìƒˆë¡œìš´ IDëŠ” ë¹ˆ ëŒ€í™”ì°½
        load_json_to_memory(memory, thread_id, filename=filename)  # JSONì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°

# âœ… ê¸°ì¡´ ì±„íŒ… ë‚´ì—­ ì¶œë ¥
for message in st.session_state["messages"]:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# âœ… ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")

if user_input:
    inputs = {"input": user_input}
    config = {"configurable": {"thread_id": thread_id}}

    # âœ… LangChain Agent ì‹¤í–‰
    response = agent.invoke(inputs, config=config)
    ai_response = response.get("output", "ì‘ë‹µì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # âœ… Memoryì— ëŒ€í™” ì €ì¥
    memory.save_context({"input": user_input}, {"output": ai_response})
    save_memory_to_json(user_input, ai_response, thread_id, filename=filename)

    # âœ… ì„¸ì…˜ ìƒíƒœì— ë©”ì‹œì§€ ì¶”ê°€
    st.session_state["messages"].append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    st.session_state["messages"].append({"role": "assistant", "content": ai_response})
    with st.chat_message("assistant"):
        st.markdown(ai_response)

    # âœ… í˜„ì¬ thread_idì˜ ëŒ€í™” ë‚´ì—­ì„ chat_historyì— ì €ì¥
    st.session_state["chat_history"][thread_id] = st.session_state["messages"]

    # âœ… ì±„íŒ…ì°½ì„ ìƒˆë¡œê³ ì¹¨í•˜ì—¬ ë°˜ì˜
    st.rerun()
