import os
import json
import openai
import streamlit as st
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langgraph.store.memory import InMemoryStore
from langchain.memory import ConversationBufferMemory
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.agents import initialize_agent, AgentType
from langchain_core.tools import tool
import random

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# âœ… JSON íŒŒì¼ì—ì„œ ëŒ€í™” ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ (ì‚¬ìš©ìë³„ Thread ê´€ë¦¬)
def load_json_to_memory(memory, thread_id, filename):
    """JSON íŒŒì¼ì—ì„œ íŠ¹ì • ì‚¬ìš©ìì˜ ëŒ€í™” ë‚´ì—­ì„ ë¶ˆëŸ¬ì™€ ConversationBufferMemoryì— ì €ì¥"""
    try:
        with open(filename, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        if thread_id not in data:
            print(f"ì‚¬ìš©ì {thread_id}ì— ëŒ€í•œ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        for conversation in data[thread_id]:
            user_input = conversation.get("user", "")
            agent_response = conversation.get("agent", "")
            if user_input and agent_response:
                memory.save_context({"input": user_input}, {"output": agent_response})
        print(f"âœ… {filename}ì—ì„œ ì‚¬ìš©ì {thread_id}ì˜ ëŒ€í™” ë‚´ì—­ì´ ë©”ëª¨ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except FileNotFoundError:
        print(f"âŒ {filename} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except json.JSONDecodeError:
        print(f"âŒ {filename} íŒŒì¼ì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")

# âœ… JSON íŒŒì¼ì— ëŒ€í™” ë‚´ì—­ ì €ì¥ í•¨ìˆ˜
def save_memory_to_json(user_input, agent_response, thread_id, filename):
    """ì‚¬ìš©ìì˜ ëŒ€í™” ë‚´ì—­ì„ JSON íŒŒì¼ì— ì¶”ê°€ ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""

    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ (íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ì½ê³ , ì—†ìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ìƒì„±)
    data = {}
    if os.path.exists(filename):
        with open(filename, "r", encoding="utf-8") as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError:
                data = {}

    # í•´ë‹¹ ì‚¬ìš©ì(thread_id)ì˜ ëŒ€í™” ê¸°ë¡ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    if thread_id not in data:
        data[thread_id] = []

    # ìƒˆë¡œìš´ ëŒ€í™” ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)
    new_entry = {"user": user_input, "agent": agent_response}
    if new_entry not in data[thread_id]:  # ì¤‘ë³µ ë°©ì§€
        data[thread_id].append(new_entry)

    # JSON íŒŒì¼ì— ì €ì¥
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

    print(f"âœ… ì‚¬ìš©ì '{thread_id}'ì˜ ëŒ€í™” ë‚´ì—­ì´ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# âœ… Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ğŸ¤– AI ì±—ë´‡", layout="wide")

# âœ… ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.title("ğŸ”‘ ì„¤ì •")
    openai_api_key = st.text_input("OpenAI API Key", type="password")
    thread_id = st.text_input("User ID")
    filename = st.text_input("Create file name")
    filename = f'{filename}.json'

if not openai_api_key or not thread_id or not filename:
    st.warning("âš ï¸ OpenAI API Keyì™€ IDì™€ ëŒ€í™”ë¥¼ ì €ì¥í•  íŒŒì¼ëª…ì„ ì…ë ¥í•˜ì„¸ìš”!")
    st.stop()

# âœ… LLM ì„¤ì •
llm = ChatOpenAI(model="gpt-4o-mini", max_tokens=200, api_key=openai_api_key)

# âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥ì†Œ
store = InMemoryStore()
checkpointer = MemorySaver()

# âœ… LangChain Memory ì„¤ì •
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key="output")
load_json_to_memory(memory, thread_id, filename=filename)  # JSONì—ì„œ ëŒ€í™” ë¶ˆëŸ¬ì˜¤ê¸°

# âœ… ê²€ìƒ‰ íˆ´
search_tool = DuckDuckGoSearchRun()

# @tool
# def memory_tool(query: str) -> str:
#     """ì‚¬ìš©ìì˜ ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì„œ ëŒ€í™”í•˜ëŠ” íˆ´"""
#     messages = memory.load_memory_variables({}).get("chat_history", "")
#     return "\n".join(messages) if messages else "ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."

@tool
def memory_tool(query: str) -> str:
    """ì‚¬ìš©ìì˜ ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì„œ ëŒ€í™”í•˜ëŠ” íˆ´"""
    messages = memory.load_memory_variables({}).get("chat_history", [])
    
    # ë©”ì‹œì§€ ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    messages_str = [msg.content for msg in messages]  # âœ… HumanMessage, AIMessage ê°ì²´ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

    return "\n".join(messages_str) if messages_str else "ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."


@tool
def search_summary_tool(query: str) -> str:
    """ì›¹ ê²€ìƒ‰ì„ í†µí•´ í•„ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” íˆ´"""
    search_results = search_tool.run(query)
    response = llm.invoke([
        {"role": "system", "content": "You are a helpful assistant that summarizes search results."},
        {"role": "user", "content": f"ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•´ì¤˜: {search_results}"}
    ])
    return response.content

@tool
def emotional_response_tool(user_input: str) -> str:
    """ì‚¬ìš©ìì˜ ê°ì •ì— ë”°ë¼ ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì‘í•˜ëŠ” íˆ´"""
    if any(word in user_input.lower() for word in ["í˜ë“¤ì–´", "ìš°ìš¸í•´", "ìŠ¬í¼", "ì§€ì³¤ì–´", "ì§œì¦ë‚˜"]):
        return random.choice(["ê´œì°®ì•„? ë¬´ìŠ¨ ì¼ ìˆì—ˆì–´?", "ìŒâ€¦ ë‚˜í•œí…Œ ë§í•´ë„ ê´œì°®ì•„. ë¬´ìŠ¨ ì¼ì¸ë°?"])
    elif any(word in user_input.lower() for word in ["ê¸°ë»", "ì¢‹ì•„", "í–‰ë³µí•´", "ì‹ ë‚˜"]):
        return random.choice(["ì˜¤! ì¢‹ì€ ì¼ì´ ìˆì—ˆêµ¬ë‚˜!", "ì™€, ë„ˆ ì •ë§ í–‰ë³µí•´ ë³´ì¸ë‹¤!"])
    return ""

# âœ… ReAct ê¸°ë°˜ ì±—ë´‡ ìƒì„±
tools=[memory_tool, search_summary_tool, emotional_response_tool]

# agent = create_react_agent(llm, tools = tools, 
#                            checkpointer=checkpointer, store=store,
#                            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # LLMì´ Taskë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ Toolì„ ì„ íƒ
#                            verbose=True)
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, 
    verbose=True,
    handle_parsing_errors = True
)


# âœ… ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ í•¨ìˆ˜ (ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µ)
def print_stream(graph, inputs, config):
    """ì²´í¬í¬ì¸íŠ¸ì™€ ë©”ëª¨ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ì§€ì†ì ì¸ ëŒ€í™”ë¥¼ ìœ ì§€"""
    for s in graph.stream(inputs, config, stream_mode="values"):
        message = s["messages"][-1]  
        if isinstance(message, tuple):
            print(message)  
        else:
            message.pretty_print()  

# âœ… Streamlit UI ìƒì„±
st.title("ğŸ¤– AI ì±—ë´‡")

# âœ… ê¸°ì¡´ ëŒ€í™” ë‚´ì—­ ì¶œë ¥
if "messages" not in st.session_state:
    st.session_state.messages = []
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# âœ… ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")
if user_input:
    print()
    print("user_input:", user_input)
    messages = memory.load_memory_variables({}).get("chat_history", "") + [("user", user_input)]
    config = {"configurable": {"thread_id": thread_id}}
    # inputs = {"messages": messages}
    inputs = {"input": messages}

    # debug
    # print(memory.load_memory_variables({}))
    # print(messages)
    # print(memory)


    # response = agent.invoke(inputs, config=config)["messages"][-1].content
    response = agent.invoke(inputs, config=config)
    # print(response)
    ai_response = response["output"]

    print("response:", ai_response, "\n")
    memory.save_context({"input": user_input}, {"output": ai_response})
    save_memory_to_json(user_input, ai_response, thread_id, filename=filename)
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)
    st.session_state.messages.append({"role": "assistant", "content": ai_response})
    with st.chat_message("assistant"):
        st.markdown(ai_response)
    st.rerun()

    print_stream(agent, inputs, config)