import os
import openai
import threading
import json
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver
from langgraph.store.memory import InMemoryStore
from langchain.memory import ConversationBufferMemory
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_core.tools import tool

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# âœ… LLM ì„¤ì • (GPT-4o ì‚¬ìš©)
llm = ChatOpenAI(model="gpt-4o-mini", max_tokens=150)

# âœ… ê¸€ë¡œë²Œ ì €ì¥ì†Œ (ê° ë‹‰ë„¤ì„ë³„ ì €ì¥)
store = InMemoryStore()

# âœ… ìœ ì €ë³„ ì²´í¬í¬ì¸íŠ¸ & ë©”ëª¨ë¦¬ ì €ì¥ì†Œ (ìŠ¤ë ˆë“œë³„ ì €ì¥)
user_checkpoints = {}
user_memories = {}

# âœ… JSON íŒŒì¼ ê²½ë¡œ
MEMORY_FILE = "./chat_memory.json"

# âœ… ëŒ€í™” ê¸°ë¡ì„ JSONìœ¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def serialize_messages(messages):
    """ë©”ì‹œì§€ ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
    return [{"type": type(msg).__name__, "content": msg.content} for msg in messages]

def deserialize_messages(messages):
    """JSONì—ì„œ ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë©”ì‹œì§€ ê°ì²´ë¡œ ë³€í™˜"""
    msg_type_map = {"HumanMessage": HumanMessage, "AIMessage": AIMessage, "SystemMessage": SystemMessage}
    return [msg_type_map[msg["type"]](content=msg["content"]) for msg in messages]

def save_data():
    """í˜„ì¬ ëª¨ë“  ìœ ì €ì˜ ëŒ€í™” ë©”ëª¨ë¦¬ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ì‹¤ì‹œê°„ ì €ì¥)"""
    data = {}

    for user, memory in user_memories.items():
        messages = memory.chat_memory.messages
        if messages:  # ğŸ”¹ ë¹„ì–´ìˆëŠ” ë°ì´í„°ëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ
            data[user] = serialize_messages(messages)

    if not data:
        print("ğŸš« ì €ì¥í•  ëŒ€í™” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    with open(MEMORY_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

    print(f"ğŸ’¾ âœ… ëŒ€í™” ê¸°ë¡ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {MEMORY_FILE}")

    # íŒŒì¼ ì €ì¥ í›„ í™•ì¸
    if os.path.exists(MEMORY_FILE):
        print("âœ… JSON íŒŒì¼ì´ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë¨!")
    else:
        print("âŒ JSON íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ!")



# âœ… JSONì—ì„œ ëŒ€í™” ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
def load_data():
    """ì €ì¥ëœ JSON íŒŒì¼ì—ì„œ ìœ ì €ë³„ ëŒ€í™” ë©”ëª¨ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜´"""
    global user_memories
    try:
        with open(MEMORY_FILE, "r", encoding="utf-8") as f:
            content = f.read().strip()
            if not content:
                print("ğŸš« ì €ì¥ëœ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
                return
            data = json.loads(content)

        for user, messages in data.items():
            user_memories[user] = ConversationBufferMemory(
                memory_key="chat_history", return_messages=True, output_key="output"
            )
            user_memories[user].chat_memory.messages = deserialize_messages(messages)
        print(f"ğŸ’¾ ğŸ”„ ëŒ€í™” ê¸°ë¡ì´ ë³µì›ë˜ì—ˆìŠµë‹ˆë‹¤: {MEMORY_FILE}")
    
    except (FileNotFoundError, json.JSONDecodeError):
        print("ğŸš« ì €ì¥ëœ ëŒ€í™” ì—†ìŒ. ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        with open(MEMORY_FILE, "w", encoding="utf-8") as f:
            json.dump({}, f, ensure_ascii=False, indent=4)  # ğŸ”¹ ë¹ˆ JSON íŒŒì¼ ìƒì„±


# âœ… í”„ë¡œê·¸ë¨ ì‹œì‘ ì‹œ ë°ì´í„° ë¡œë“œ
load_data()

# âœ… ê²€ìƒ‰ íˆ´ (ì‹¤ì‹œê°„ ì •ë³´ ì œê³µ)
search_tool = DuckDuckGoSearchRun()

# ğŸ”¹ **ëŒ€í™” ê¸°ì–µ íˆ´ (ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ)**
@tool
def memory_tool(query: str) -> str:
    """ì‚¬ìš©ìì˜ ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì„œ ëŒ€í™”í•˜ëŠ” íˆ´"""
    thread_id = threading.current_thread().name  # í˜„ì¬ ìŠ¤ë ˆë“œ(ë‹‰ë„¤ì„) ê°€ì ¸ì˜¤ê¸°
    memory = user_memories.get(thread_id)  # í•´ë‹¹ ìœ ì €ì˜ ë©”ëª¨ë¦¬ ê°€ì ¸ì˜¤ê¸°

    if memory:
        messages = memory.chat_memory.messages  # âœ… ì €ì¥ëœ ëŒ€í™” ê°€ì ¸ì˜¤ê¸°
        for msg in reversed(messages):
            if query.lower() in msg.content.lower():
                return msg.content  # âœ… ìì—°ìŠ¤ëŸ½ê²Œ í•´ë‹¹ ë‚´ìš©ì„ ëŒ€í™”ì— ë…¹ì—¬ì„œ ì‚¬ìš©
    return ""

# ğŸ”¹ **ê²€ìƒ‰ íˆ´ (ì›¹ì—ì„œ ì •ë³´ ê²€ìƒ‰ í›„ ìš”ì•½)**
@tool
def search_summary_tool(query: str) -> str:
    """ì›¹ ê²€ìƒ‰ì„ í†µí•´ í•„ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” íˆ´"""
    search_results = search_tool.run(query)
    response = llm.invoke([
        {"role": "system", "content": "You are a helpful assistant that summarizes search results."},
        {"role": "user", "content": f"ë‹¤ìŒ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•´ì¤˜: {search_results}"}
    ])
    return response.content  # âœ… ë¶ˆí•„ìš”í•œ ë¼ë²¨ ì œê±°

# âœ… ì‚¬ìš© ê°€ëŠ¥í•œ íˆ´ ëª©ë¡
tools = [memory_tool, search_summary_tool]

# âœ… ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ í•¨ìˆ˜ (ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µ)
def print_stream(graph, inputs, config):
    """ì²´í¬í¬ì¸íŠ¸ì™€ ë©”ëª¨ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ì§€ì†ì ì¸ ëŒ€í™”ë¥¼ ìœ ì§€"""
    for s in graph.stream(inputs, config, stream_mode="values"):
        message = s["messages"][-1]
        if isinstance(message, tuple):
            print(message)
        else:
            message.pretty_print()

# âœ… ì±—ë´‡ ì‹¤í–‰ í•¨ìˆ˜ (ë‹‰ë„¤ì„ë³„ ëŒ€í™” ê¸°ì–µ)
def run_agent():
    thread_id = input("\nâœ… ë‹¹ì‹ ì˜ ë‹‰ë„¤ì„ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()

    # âœ… ë‹‰ë„¤ì„ë³„ ê°œë³„ ì²´í¬í¬ì¸íŠ¸ & ë©”ëª¨ë¦¬ ìƒì„±
    if thread_id not in user_checkpoints:
        user_checkpoints[thread_id] = MemorySaver()
        user_memories.setdefault(thread_id, ConversationBufferMemory(
            memory_key="chat_history", return_messages=True, output_key="output"
        ))

    checkpointer = user_checkpoints[thread_id]
    memory = user_memories[thread_id]

    # âœ… ë‹‰ë„¤ì„ë³„ ê°œë³„ graph ìƒì„±
    graph = create_react_agent(llm, tools=tools, checkpointer=checkpointer, store=store)

    # âœ… í˜„ì¬ ìŠ¤ë ˆë“œ(ì‚¬ìš©ì ë‹‰ë„¤ì„) ì €ì¥
    threading.current_thread().name = thread_id

    config = {"configurable": {"thread_id": thread_id}}
    graph.get_state(config)

    while True:
        user_input = input("\nì‚¬ìš©ì ì…ë ¥: ").strip()

        # âœ… "ì»¤ë„ ì¢…ë£Œ" ì…ë ¥ ì‹œ í”„ë¡œê·¸ë¨ ì¢…ë£Œ
        if user_input.lower() in ["ì»¤ë„ ì¢…ë£Œ", "kernel shutdown"]:
            print("ğŸ›‘ ì»¤ë„ ì¢…ë£Œ ìš”ì²­ë¨. ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•˜ê³  ì¢…ë£Œí•©ë‹ˆë‹¤.")
            save_data()
            os._exit(0)  # í”„ë¡œê·¸ë¨ ê°•ì œ ì¢…ë£Œ

        # âœ… "ê·¸ë§Œ", "ì¢…ë£Œ", "quit" ì…ë ¥ ì‹œ JSON íŒŒì¼ ì‹¤ì‹œê°„ ì €ì¥ í›„ ì¢…ë£Œ
        elif user_input.lower() in ["ê·¸ë§Œ", "ì¢…ë£Œ", "quit"]:
            print("ğŸ›‘ ëŒ€í™” ì¢…ë£Œ")
            save_data()  # ğŸ”¹ ì‹¤ì‹œê°„ ì €ì¥
            os._exit(0)

        # âœ… LLM ì‹¤í–‰ í›„ ì‘ë‹µ ì €ì¥
        inputs = {"messages": [("user", user_input)]}
        response = graph.invoke(inputs, config=config)

        # âœ… ìœ ì €ë³„ LangChain ë©”ëª¨ë¦¬ì— ëŒ€í™” ì €ì¥
        memory.save_context({"input": user_input}, {"output": response["messages"][-1].content})

        # âœ… ëŒ€í™”ê°€ ëë‚  ë•Œë§ˆë‹¤ JSON íŒŒì¼ ìë™ ì—…ë°ì´íŠ¸ (ì‹¤ì‹œê°„ ì €ì¥)
        save_data()

        print_stream(graph, inputs, config)

# âœ… ì‹¤í–‰ (ë©€í‹°ìŠ¤ë ˆë“œ ì§€ì›)
if __name__ == "__main__":
    while True:
        thread = threading.Thread(target=run_agent)
        thread.start()
        thread.join()
