import streamlit as st
import openai
import json
import os
from dotenv import load_dotenv
from langchain.memory import ConversationBufferMemory
from langchain.chat_models import ChatOpenAI

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (OPENAI API í‚¤)
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# âœ… JSON íŒŒì¼ì— ëŒ€í™” ë‚´ì—­ ì €ì¥
CHAT_HISTORY_FILE = "chat_history.json"

def save_memory_to_json(memory, filename=CHAT_HISTORY_FILE):
    """ConversationBufferMemoryì˜ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    data = memory.load_memory_variables({})
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

def load_memory_from_json(memory, filename=CHAT_HISTORY_FILE):
    """JSON íŒŒì¼ì—ì„œ ëŒ€í™” ë‚´ì—­ì„ ë¶ˆëŸ¬ì™€ ConversationBufferMemoryì— ì €ì¥"""
    if os.path.exists(filename):
        with open(filename, "r", encoding="utf-8") as f:
            data = json.load(f)
        if "history" in data:
            messages = data["history"].split("\n")
            for i in range(0, len(messages), 2):
                if i + 1 < len(messages):
                    memory.save_context(
                        {"input": messages[i].replace("Human: ", "").strip()},
                        {"output": messages[i+1].replace("AI: ", "").strip()},
                    )

# âœ… Streamlit UI ì„¤ì •
st.set_page_config(page_title="LangChain Chatbot", layout="wide")
st.title("ğŸ¤– ChatGPT ìŠ¤íƒ€ì¼ ì±—ë´‡")

# âœ… ëŒ€í™” ê¸°ë¡ ì €ì¥ì„ ìœ„í•œ LangChain Memory
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory()
    load_memory_from_json(st.session_state.memory)  # JSONì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°

memory = st.session_state.memory

# âœ… LLM ëª¨ë¸ ì„¤ì • (GPT-4o ì‚¬ìš©)
llm = ChatOpenAI(model_name="gpt-4o", temperature=0.7)

# âœ… Streamlit ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
st.write("ğŸ’¬ **ì±—ë´‡ê³¼ ëŒ€í™”í•˜ì„¸ìš”!**")

# âœ… ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì´ì „ ëŒ€í™” í‘œì‹œ
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...")

if user_input:
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # GPT ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        response = llm.predict(user_input)
        st.markdown(response)

    # LangChain Memoryì— ì €ì¥
    memory.save_context({"input": user_input}, {"output": response})
    
    # JSON íŒŒì¼ì—ë„ ì €ì¥
    save_memory_to_json(memory)

    # ëŒ€í™” ë‚´ì—­ ê°±ì‹ 
    st.session_state.messages.append({"role": "assistant", "content": response})
